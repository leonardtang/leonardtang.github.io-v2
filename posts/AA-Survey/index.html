<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>A Survey on Adversarial Attacks | Leonard Tang</title><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="A Survey on Adversarial Attacks" /><meta name="author" content="Leonard Tang" /><meta property="og:locale" content="en_US" /><meta name="description" content="Leonard Tang’s personal website." /><meta property="og:description" content="Leonard Tang’s personal website." /><link rel="canonical" href="https://leonardtang.me/posts/AA-Survey/" /><meta property="og:url" content="https://leonardtang.me/posts/AA-Survey/" /><meta property="og:site_name" content="Leonard Tang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-04-06T10:20:00-04:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="A Survey on Adversarial Attacks" /><meta name="twitter:site" content="@leonardtang_" /><meta name="twitter:creator" content="@Leonard Tang" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Leonard Tang’s personal website.","headline":"A Survey on Adversarial Attacks","dateModified":"2021-04-06T10:20:00-04:00","datePublished":"2021-04-06T10:20:00-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://leonardtang.me/posts/AA-Survey/"},"url":"https://leonardtang.me/posts/AA-Survey/","author":{"@type":"Person","name":"Leonard Tang"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script> <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/Leonard-Tang.JPG" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Leonard Tang</a></div><div class="site-subtitle font-italic">A slice of my mind.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>ALL POSTS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>EXPLORE</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/leonardtang" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/leonardtang_" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:window.open('mailto:' + ['leonardtang','college.harvard.edu'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" > <i class="fas fa-rss"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>A Survey on Adversarial Attacks</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>A Survey on Adversarial Attacks</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 6, 2021, 10:20 AM -0400" > Apr 6, 2021 <i class="unloaded">2021-04-06T10:20:00-04:00</i> </span> by <span class="author"> Leonard Tang </span></div><div> Updated <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 27, 2021, 2:27 AM -0400" > Apr 27, 2021 <i class="unloaded">2021-04-27T02:27:34-04:00</i> </span></div></div><div class="post-content"> <script src="//yihui.org/js/math-code.js"></script> <script async="" src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><p>Breaking AI.</p><hr /><h1 id="attacking-machine-learning"><strong>Attacking Machine Learning</strong></h1><p>Adversarial attacks have gotten no shortage of hype in the machine learning community throughout the past few years. Their intrigue stems from the fact they are much like optical illusions for machine learning models. More precisely, adversarial attacks are carefully (i.e. mathematically) crafted inputs in a model’s domain that are benign to humans but are adversarial to machines; that is, models misclassify such inputs in spectacular fashion. For concreteness, take a look at one of the first examples of adversarial attacks for computer vision:</p><center><figure font-style="italic"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/fgsm-image.png" alt="FGSM Attack" width="700" /><figcaption> Fig. 1 - An adversarial attack fools an image classifier into thinking a panda is a gibbon (Goodfellow et al., 2014)</figcaption></figure></center><p>While adversarial attacks are fun to play with, their existence is not a mere curiosity. The fact that adversarial attacks can fool supposedly sophisticated deep learning models highlights fundamental flaws in the state-of-the-art in machine learning. In some sense, the existence of adversarial attacks suggests that such models are nothing more than glorified, brittle pattern matchers. Specifically, deep learning classifiers, as they stand today, more or less view some set of input pixel values and magically learn to pattern match them to classes; however, they lack the ability to store generalized, robust knowledge – that is, not just the ability to derive some complex mapping between input and output, but to understand <em>what</em> about the input induces distinctive, invariant properties that are characteristic of a certain class. Without this fundamental, human-like ability to intuit generalized features, machine learning will continue to be thwarted by adversarial attacks.</p><p>In this post, we’ll take a tour of the vast field of adversarial attacks and robustness, and see how and why deep learning can be broken.</p><h1 id="the-basics-terminology"><strong>The Basics: Terminology</strong></h1><p><strong>Adversarial Attack:</strong> model input that is adversarial in the sense that it seeks to cause unintended, nonsensical behavior in the model. In computer vision, specifically image classification, this is often a “minimally perturbed” image – that is, an image that appears to be unedited to the human eye, but causes gross misclassification by the machine.</p><p><strong>Attack Surface:</strong> the domain in which the adversary will modify elements. Generally, the adversary will manipulate either collection or processing of data.</p><h2 id="types-of-attacks"><strong>Types of Attacks</strong></h2><p><strong>Poisoning Attack:</strong> an adversary contaminates the training data by inserting well-chosen samples into the training process.</p><p><strong>Exploratory Attack:</strong> an adversary queries a black box model to explore its behavior.</p><p><strong>Evasion Attack:</strong> an adversary creates malicious samples during testing phase. It does <em>not</em> touch the training data. This it what we generally think of when it comes to adversarial attacks.</p><p><strong>Training Phase Attacks:</strong> an adversary attempts to manipulate training labels in a clever way to cause misclassification at test time. Alternatively, malicious data might be injected into the dataset during training. Neither of these approaches are that common in the literature nor in practice.</p><p><strong>Testing Phase Attacks (Generations):</strong></p><p>In general, given a model $ f $ and input $ X $, the goal is to craft $ X’ = X + \delta X$ where $ \delta X$ is some perturbation such that $f(X’) = Y \neq f(X)$. We focus first on the <strong>whitebox attack</strong> scenario, where the adversary has full knowledge of the model (i.e. parameters, architecture, and losses).</p><h2 id="adversarial-capabilities-during-training"><strong>Adversarial Capabilities During Training</strong></h2><p><strong>Data Injection:</strong> an adversary can insert adversarial samples during training, but has no access to the training data or model.</p><p><strong>Data Modification:</strong> an adversary can modify the training data before it gets sent to algorithm. Adversary has access to full training dataset but not model.</p><p><strong>Logic Corruption:</strong> adversary directly attacks model logic instead of the training data.</p><h2 id="white-vs-black-box-attacks"><strong>White vs. Black-Box Attacks</strong></h2><p><strong>White Box Attacks:</strong> in this setting, the adversary has complete knowledge of the target model’s architecture, its training data, and its parameters (i.e. its current weights, and hence the losses throughout the model).</p><p><strong>Black Box Attacks:</strong> here, the adversary has <em>no</em> knowledge of the model. It does, however, have the ability to sample inputs from the training distribution. Thus, it can query the model with inputs and observe outputs.</p><ul><li><strong>Non-Adaptive Black Box Attacks:</strong> here, the adversary has access to the training data distribution and transfers white box attacks on a surrogate model approximating the target model. Specifically, one can train a local model with architecture $ f’ $ on the original data distribution $ \mu $. $ f’ $ behaves similarly to the original model $f$, and thus it is reasonable to craft adversarial inputs for $ f’ $ using white-box strategies, then transfer them as attacks on $f$.</li><li><strong>Adaptive Black Box Attacks:</strong> here, the adversary has <em>no</em> information about the training dataset <em>or</em> the model. Thus, it must treat the target model as an oracle, adaptively querying the model with non-training-distribution data $ X $ to obtain labels $ Y $. The adversary trains a surrogate model with architecture $ f’ $ over tuples $ (X,Y)$. Again, the adversary crafts white-box attacks on $f’$, and transfers them over to the target model $ f $. It is an “adaptive” approach in the sense that the queries $ X $ are chosen “cleverly” over time to uncover the behavior of the underlying model.</li><li><strong>Strict Black Box Attack:</strong> here, the adversary can also collect tuples $(X,Y)$ by querying the target model. However, unlike adaptive attacks, these models cannot change inputs to observe outputs.</li></ul><p>Below is a table summarizing the key differences between white and black box attacks:</p><center><figure font-style="italic"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/black-vs-white-attack.png" alt="White vs. Black Box Attacks" width="700" /><figcaption> Fig. 2 - Table Summarizing Differences Between White and Black Box Attacks</figcaption></figure></center><h1 id="framework-of-a-test-time-adversary"><strong>Framework of a Test-Time Adversary</strong></h1><p>A typical test-time whitebox attack can be thought of as two subprocesses:</p><ol><li>First, the adversary will estimate the <strong>directional sensitivity</strong> of the data. That is, the adversary aims to get a grasp of which direction(s) in the data manifold could potentially induce class change. Generally, this amounts to obtaining the gradient of the loss function with respect to the input image.</li><li>Second, given this sensitivity information, the adversary needs to actually perturb the input image. There are a variety of ways to approach this, which we will cover later in this write-up.</li></ol><p>The adversary continues to iteratively perform the above process until the input image is sufficiently perturbed for misclassification. Ideally, the adversary should minimize iterations and perturbations to create a minimally-observable change to the input image that avoids human detection.</p><p>Formally, adversarial samples can be considered as a solution to the following optimization problem. That is, for some norm $ | \cdot |$, we want to solve the following:</p>\[X' = X + \underset{\delta X}{\mathop{\mathrm{argmin}}} \{ \| \delta X\| : f(X + \delta X) \neq f(X) \}\]<p>Solving this directly in practice is difficult since this is a non-convex formulation for neural networks. So, we approximate using the approaches outlined below.</p><h1 id="gradient-ascent-based-attack-variations"><strong>Gradient Ascent-Based Attack Variations</strong></h1><h2 id="general-fast-gradient-sign-method-fgsm"><strong>General Fast Gradient Sign Method (FGSM)</strong></h2><p>FGSM is a very fast, one-iteration, gradient-based attack method. Recall that the gradient of the loss function w.r.t. the input image points us in the direction of greatest ascent on the data manifold. Here, instead of stepping in the direction of the gradient, we step in the direction of the <em>sign</em> of the gradient:</p>\[\eta = \epsilon \cdot \text{sign}(\nabla_x J_\theta (x,l))\] \[x' = x + \eta\]<p>The $ \epsilon $ ensures the resulting $x’$ is still within an $\epsilon$-ball (as defined for $ | \cdot |_\infty $) around $x$.</p><h2 id="fast-gradient-method-fgm"><strong>Fast Gradient Method (FGM)</strong></h2><p>FGM is essentially the same as FGSM, except the image is perturbed in the direction of the gradient, i.e. not the sign of the gradient:</p>\[x' = x + \epsilon \cdot \frac{\nabla_x J_\theta(x, l)}{\|\nabla_x J_\theta(x, l)\|_p}\]<p>The gradient is normalized to ensure that the perturbation is still within the $\epsilon$-bound.</p><h2 id="targeted-fast-gradient-sign-method"><strong>Targeted Fast Gradient Sign Method</strong></h2><p>The approach above forces misclassification <em>away</em> from the starting class, but does not specify <em>which</em> class the image should move towards. Here, define the target class to be $l’ \neq l$, the original class. Then the approach is as follows:</p>\[x' = x - \epsilon \cdot (\nabla_x J_\theta (x, l'))\]<p>Here, the image is stepping <em>towards</em> a specific incorrect class, rather than merely away from the correct class (in an untargeted fashion).</p><h2 id="iterative-fast-gradient-sign-method-i-fgsm"><strong>Iterative Fast Gradient Sign Method (I-FGSM)</strong></h2><p>The idea here is to simply iteratively perform the FGSM:</p>\[x_{t+1} = x_t + \epsilon \cdot \alpha \cdot \text{sign}(\nabla_x J_\theta (x_t,l)) \text{, where} x_0 = x\]<p>Specifically, clip $x_{t+1}$ into an $\epsilon$-ball around $x_0$ after each iteration to minimize human detection. Alternatively, it also possible to set $\alpha = \epsilon /T$, where $T$ is the number of iterations.</p><p>Generally, iterative methods are more successful at confusing classifiers due to their more nuanced approach towards the mistargeted class.</p><p>However, on the flip side, more iterations necessarily require longer runtimes to generate attacks due. Surprisingly, they also do not transfer as well across models as their single-step counterparts.</p><h2 id="momentum-iterative-fast-gradient-sign-method-mi-fgsm"><strong>Momentum Iterative Fast Gradient Sign Method (MI-FGSM):</strong></h2><p>The approach here is largely the same as above, barring the introduction of a momentum vector! Specifically, accumulate a velocity vector $g$ over time with some decay factor $\mu$. To account for varying gradient scales across iterations, normalize the gradients during each iteration:</p>\[g_{t+1} = \mu \cdot g_t + \frac{\nabla_x J_\theta(x_t, l)}{\|\nabla_x J_\theta(x_t, l)\|_p}\]<p>Then step in the direction of the sign of the velocity vector:</p>\[x_{t+1} = \alpha \cdot \text{sign}(g_{t+1})\]<h2 id="projected-gradient-descent-pgd"><strong>Projected Gradient Descent (PGD):</strong></h2><p>Technically, projected gradient descent should be called projected gradient <em>ascent</em>. It is a natural extension of FGSM and is fundamentally the same idea as I-FGSM.</p><p>The approach here is to step in the direction of the sign of the gradient for multiple iterations while projecting the vector at each iteration into an $\epsilon$-ball around the original data point:</p>\[x_{t + 1} = \pi(x_t + \alpha \cdot \text{sign}(\nabla_x J_\theta (x_t, l)))\]<p>Here, $\pi$ is some kind of projection. In the $l_\infty$-norm case, a simple clipping function suffices; in PyTorch, this would be something like <code class="language-plaintext highlighter-rouge">torch.clamp</code>.</p><p>To better explore the loss landscape of first-order attacks, the authors in <a href="https://arxiv.org/abs/1706.06083">Madry et. al (2017)</a> start PGD from many points in the $l_\infty$-balls around the input data points. Interestingly enough, though the local maxima are spread all throughout these $\epsilon$-balls, their <em>values</em> are tightly concentrated. This, alongside strong experimental evidence – PGD-attack-trained models were consistently robust against all other gradient-ascent-based methods – that PGD is a universal first-order adversary, i.e. the strongest possible attack only leveraging gradient information. That is, a model that is robust to PGD attacks is robust to all first-order attacks.</p><h1 id="other-attacks"><strong>Other Attacks</strong></h1><h2 id="limited-memory-broydenfletchergoldfarbshanno-attack-l-bfgs"><strong>Limited Memory Broyden–Fletcher–Goldfarb–Shanno Attack (L-BFGS):</strong></h2><p>L-BFGS itself is a well-known quasi-newton optimization method, i.e. a second order optimization method with Hessians approximated based on the $k$-most recent gradients.</p><p>L-BFGS was one of the very first adversarial attack approaches. It exhibits decent attack capabilities but unfortunately is very expensive.</p><h2 id="deepfool"><strong>DeepFool:</strong></h2><p>DeepFool is a non-gradient-ascent based iterative algorithm for finding adversarial images. The core idea is to start with an input image, search for the closest decision boundary (the specific boundary is irrelevant), orthogonally project the input image to a linear approximation of the decision boundary, and repeat until the perturbed image has crossed a boundary.</p><p>The full algorithm for the multi-class instance is shown below:</p><center><figure font-style="italic"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/deepfool-algo.png" alt="White vs. Black Box Attacks" width="700" /><figcaption> Fig. 3 - The DeepFool Algorithm</figcaption></figure></center><h2 id="jsma-jacobian-based-saliency-map-attack"><strong>JSMA: Jacobian-Based Saliency Map Attack</strong></h2><p>The idea behind JSMA is to compute a saliency map output $S^{+}(x_i,l)$ for a model/image/class combination (here $x_i$ is one pixel of the input $x$). Recall that the saliency map roughly calculates the correlation (positive, for a $S^+$ map) between an input pixel and the predicted class.</p><p>Now theoretically, by increasing a few high-saliency pixels $x_i$ according to $S^{+}(x_i, l=target)$, the perturbed image will be more likely to be classified as the adversarial class <em>target</em>.</p><p>In practice, this approach actually searches over salient pixel <em>pairs</em> due to the strict nature of the salience map criterion. However, the core idea is the same.</p><h2 id="backward-pass-differentiable-approximation-attack"><strong>Backward Pass Differentiable Approximation Attack:</strong></h2><p>Conceptually, this attack is similar in principle to the idea of a Straight Through Estimator. In fact, using Straight Through Estimation is a special case of this Differentiable Approximation Attack.</p><p>This attack rose to prominence as a response to model defenses that obfuscated gradients via the introduction of a non-differentiable layer $f^i(\cdot)$. Clearly, standard first-order approaches wouldn’t succeed given the non-differentiability.</p><p>However, it is possible to first find a differentiable approximation $g(x)$ such that $g(x) \approx f^i(\cdot)$. Then, using $g$ we can approximate $\nabla_x f(x)$ by first forward passing through $f(\cdot) = f^{1\dots j}(\cdot)$ but replacing $f^i(\cdot)$ with $g(\cdot)$ on the backward pass. This effectively bypasses the non-differentiability, nullifying the defense and allowing the power of first-order methods to flourish.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/explanations/'>Explanations</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/academic/" class="post-tag no-text-decoration" >academic</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=A Survey on Adversarial Attacks - Leonard Tang&url=https://leonardtang.me/posts/AA-Survey/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=A Survey on Adversarial Attacks - Leonard Tang&u=https://leonardtang.me/posts/AA-Survey/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=A Survey on Adversarial Attacks - Leonard Tang&url=https://leonardtang.me/posts/AA-Survey/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Life-Gradient-Descent/">Life is Gradient Descent: A Plea Against Short-Term Overoptimization</a></li><li><a href="/posts/Stonks/">Stonks, Stonks, Stonks</a></li><li><a href="/posts/AA-Survey/">A Survey on Adversarial Attacks</a></li><li><a href="/posts/ToC/">Theory of Computation is Fun</a></li><li><a href="/posts/CNNs-NLP/">CNNs for NLP</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/academic/">academic</a> <a class="post-tag" href="/tags/ruminations/">ruminations</a> <a class="post-tag" href="/tags/projects/">projects</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Fall-2020-Notes/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Aug 26, 2020 <i class="unloaded">2020-08-26T03:14:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Fall 2020 Course Notes</h3><div class="text-muted small"><p> A Collection of My Fall 2020 Course Notes: An * denotes a graduate level course, and a † denotes a class I am a Course Assistant for. Math 122: Theory of Groups and Vector Spaces AC 209a: ...</p></div></div></a></div><div class="card"> <a href="/posts/ToC/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Dec 18, 2020 <i class="unloaded">2020-12-18T10:12:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Theory of Computation is Fun</h3><div class="text-muted small"><p> I might just become a Complexity Theorist. First, A Synopsis of 18.404/6.840 This class was…strange. Incredibly challenging and intellectually stimulating, though quite unlike any analytical ...</p></div></div></a></div><div class="card"> <a href="/posts/ConvSort/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Oct 28, 2021 <i class="unloaded">2021-10-28T02:15:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ConvSort</h3><div class="text-muted small"><p> Let’s solve an easy algorithmic task with the most convoluted (pun intended) approach possible. Why? Because we can. Background Consider the canonical algorithmic task of sorting an array of nu...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Gumbel-Max/" class="btn btn-outline-primary"><p>On the Gumbel-Max Trick</p></a> <a href="/posts/Stonks/" class="btn btn-outline-primary"><p>Stonks, Stonks, Stonks</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/leonardtang_">Leonard Tang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/academic/">academic</a> <a class="post-tag" href="/tags/ruminations/">ruminations</a> <a class="post-tag" href="/tags/projects/">projects</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leonardtang.me{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
